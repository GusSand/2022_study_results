# 2022 Study on the security implications of Large Language Model Code Assistants

This repository contains the results of the 2022 study described in the Paper: `Lost at C: A User Study on the security implications of Large Language Model Code Assistants`  Link: https://arxiv.org/pdf/2208.09727.pdf

## To reproduce the results
- create a virtual environment with python 3.8 or above
- activate the virtual environment
- install the requirements with `pip install -r requirements.txt`


## Data
The data folder contains (1) all the `submitted_assignments` folder which contains the raw data from the study per study participant (study files, etc); (2) the `derived_data` folder which contains files generated by the analysis scripts, and (3) the loose files manually produced during the study.

These are described here:

- `active_inactive.txt` - Links each study participant's ID to their assigned group.
- `bug_finding_flat.csv` - A "flattened" version of the annotated bug location database in CSV format.
- `bug_finding.json` - The annotated bug location database in JSON format.
- `bugs_and_demographics.sqlite3` - The canonical bug location database as well as anonymous study participant demographics.
- `llm_log_dump.json` - The log of all suggestions generated by the LLM and associated metadata. 
- `histogram_data_study.csv` - The log for how long each participant took to complete the study.

## Code
The code used to generate the figures on the paper is in the following files:

### Figures

#### Figure 7:

This figure is generated with a single script, as follows:

```
python plot_fig7.py
```

This script uses the data from the `data/submitted_assignments` folder to generate figure 7 in the paper. 
The output file is called `figures/functionality.pdf`. It also generates CSVs which may be inspected, `data/derived_data/functional_tests.csv` and `data/derived_data/functionality_stats.txt`.

#### Figure 8:

This figure is generated with a single script, as follows:

```
python plot_fig8.py
```

This script uses the data from the data folder to generate figure 8 which contains 4 subfigures. 
2 figures for bugs per lines of code that compile: `figures/bugs_per_loc_compiled.pdf` and `figures/bugs_per_loc_severe_compiled.pdf`.
In additions there are also two figures for lines of code that pass tests: `figures/bugs_per_loc_passing.pdf` and `figures/bugs_per_loc_severe_passing.pdf`.

This script also generates 4 CSV files with the data used to generate the statistical tests, the files will be created in `data/derived_data/`. 

To run the statistical tests, run the following scripts:

```
inferiority_tests.py
```
This will print the results of the non-inferiority tests to the terminal.


#### Figure 9:

This figure is generated with a single script, as follows:

```
python plot_fig9.py
``` 

This script uses the data from the data folder to generate figure 9. 
The output file is `figures/cwe_prevalence.pdf`.

#### Figure 12 (In the Appendix):

This figure is generated with a single script, as follows:

```
python plot_time_data.py
``` 

This script uses the data from the data folder to generate a histogram for the time that the students took to finalize the study. 
The output file is `figures/histogram_of_study_duration.pdf`.

### Tables

#### Table 3:

This table is made in two passes.

Firstly we collect the contents of the table's rows by running
```
python generate_table3.py
```
This produces `data/derived_data/table3.tsv`.

Then, we perform the non-inferiority tests by running

```
python inferioririty_per_func.py
```

This prints the results of the non-inferiority tests to the terminal.

#### Table 4:

This table has two rows, LoC and Bugs. The code to generate the data for this table is in the following files:

- LoC: `suggestion_cover.py` - This creates the HTML visualization showing which portions of the code were human written and which were suggested by the code assistant. To run:

    ```
    python suggestion_cover.py -o data/derived_data/suggestion_cover.html
    ```

    This creates the files `data/derived_data/suggestion_cover.html` and `data/derived_data/suggestion_cover.jsonl`. At the end of the script, it also prints out the coverage statistics, which are used for the LoC line in Table 4:

    ```
    Summary:

    Coverage including template lines:
    51% Template
    30% Human
    8% Codex (exact)
    9% Codex (approx)

    Coverage excluding template lines:
    63% Human
    17% Codex (exact)
    19% Codex (approx)
    ```

- Bugs: `bug_origin_all.py` - This script extracts the annotations for each bug and saves them in JSONL format in `data/derived_data/bug_origin_all.jsonl`. To run:

    ```
    python bug_origin_all.py
    ```

    It also prints out the statistics for the Bugs line in Table 4:

    ```
    Bug origin counts:
    Human:           356 (63.1%)
    Codex (exact):    92 (16.3%)
    Codex (approx):  112 (19.9%)
    Unknown:           4 ( 0.7%)

    Unrecorded:       55
    ```

#### Table 5:

This table is generated by a single script, `bug_origin_cwe416.py`. This prints a CSV to stdout with the bug origin data for the CWE-416 "strdup" bug, which is presented in the paper in Table 5. To run:

```
python bug_origin_cwe416.py
```

The output is:

```
uuid,origin,num_bug_in_all_suggestions,num_bugs_in_taken_suggestions,bug_count_in_final_doc
0640,Suggestion,5,3,3
1f1c,Document,5,0,2
26a4,Suggestion,3,1,2
[...]
```

#### Table 7:
